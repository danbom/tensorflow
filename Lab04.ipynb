{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab04.ipynb",
      "provenance": [],
      "private_outputs": true,
      "authorship_tag": "ABX9TyN7zE0d/t5XrFgFAdfwHE/r",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danbom/tensorflow/blob/master/Lab04.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BjoQ6dwGDjLe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "tf.random.set_seed(0)\n",
        "\n",
        "x1_data = [1, 0, 3, 0, 5]\n",
        "x2_data = [0, 2, 0, 4, 0]\n",
        "y_data = [1, 2, 3, 4, 5]\n",
        "\n",
        "W1 = tf.Variable(tf.random.uniform((1,), -10.0, 10.0))\n",
        "W2 = tf.Variable(tf.random.uniform((1,), -10.0, 10.0))\n",
        "b = tf.Variable(tf.random.uniform((1,), -10.0, 10.0))\n",
        "\n",
        "learning_rate = tf.Variable(0.001)\n",
        "\n",
        "for i in range(1000+1) :\n",
        "    with tf.GradientTape() as tape:\n",
        "        hypothesis = W1 * x1_data + W2 * x2_data + b\n",
        "        cost = tf.reduce_mean(tf.square(hypothesis - y_data))\n",
        "    W1_grad, W2_grad, b_grad = tape.gradient(cost, [W1, W2, b])\n",
        "    W1.assign_sub(learning_rate * W1_grad)\n",
        "    W2.assign_sub(learning_rate * W2_grad)\n",
        "    b.assign_sub(learning_rate * b_grad)\n",
        "\n",
        "    if i % 50 == 0 :\n",
        "        print(\"{:5} | {:10.6f} | {:10.4f} | {:10.4f} | {:10.6f}\".format(i, cost.numpy(), W1.numpy()[0], W2.numpy()[0], b.numpy()[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xB4qwq9sI1ix",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_data = [[1., 0., 3., 0., 5.],[0., 2., 0., 4., 0.]]\n",
        "y_data = [1, 2, 3, 4, 5]\n",
        "\n",
        "W = tf.Variable(tf.random.uniform((1,2),-1.0,1.0))\n",
        "b = tf.Variable(tf.random.uniform((1,),-1.0,1.0))\n",
        "\n",
        "learning_rate = tf.Variable(0.001)\n",
        "\n",
        "for i in range(1000+1):\n",
        "    with tf.GradientTape() as tape :\n",
        "        hypothesis = tf.matmul(W, x_data) + b\n",
        "        cost = tf.reduce_mean(tf.square(hypothesis - y_data))\n",
        "\n",
        "    W_grad, b_grad = tape.gradient(cost, [W,b])\n",
        "    W.assign_sub(learning_rate * W_grad)\n",
        "    b.assign_sub(learning_rate * b_grad)\n",
        "\n",
        "    if i % 50 == 0 :\n",
        "        print(\"{:5} | {:10.6f} | {:10.4f} | {:10.4f} | {:10.6f}\".format(\n",
        "            i, cost.numpy(), W.numpy()[0][0], W.numpy()[0][1], b.numpy()[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qGpMj5xgLe0J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}