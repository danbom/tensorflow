{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lab07.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNvF8P4C6//+uofpiADCUAX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danbom/tensorflow/blob/master/lab07.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7lHjJAioSlAT",
        "colab_type": "text"
      },
      "source": [
        "lab 07 learning_rate_and_evaluation - Eager execution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_BOWiMJGSVPy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "tf.random.set_seed(777)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Fm0GYDkTLKq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train = [[1,2,1],\n",
        "           [1,3,2],\n",
        "           [1,3,4],\n",
        "           [1,5,5],\n",
        "           [1,7,5],\n",
        "           [1,2,5],\n",
        "           [1,6,6],\n",
        "           [1,7,7]]\n",
        "y_train = [[0, 0, 1],\n",
        "          [0, 0, 1],\n",
        "          [0, 0, 1],\n",
        "          [0, 1, 0],\n",
        "          [0, 1, 0],\n",
        "          [0, 1, 0],\n",
        "          [1, 0, 0],\n",
        "          [1, 0, 0]]\n",
        "\n",
        "x_test = [[2, 1, 1],\n",
        "          [3, 1, 2],\n",
        "          [3, 3, 4]]\n",
        "y_test = [[0, 0, 1],\n",
        "          [0, 0, 1],\n",
        "          [0, 0, 1]]\n",
        "\n",
        "x1 = [x[0] for x in x_train]\n",
        "x2 = [x[1] for x in x_train]\n",
        "x3 = [x[2] for x in x_train]\n",
        "\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "ax.scatter(x1,x2,x3, c=y_train, marker='^')\n",
        "\n",
        "ax.scatter(x_test[0][0],x_test[0][1], x_test[0][2], c='black', marker='^')\n",
        "ax.scatter(x_test[1][0],x_test[1][1], x_test[1][2], c='black', marker='^')\n",
        "ax.scatter(x_test[2][0],x_test[2][1], x_test[2][2], c='black', marker='^')\n",
        "\n",
        "ax.set_xlabel('X Label')\n",
        "ax.set_ylabel('Y Label')\n",
        "ax.set_zlabel('Z Label')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xuFVqUHHUkTB",
        "colab_type": "text"
      },
      "source": [
        "위 Data를 기준으로 Learning rate 값과 평가 모델 만들기\n",
        "- Tensorflow data API를 통해 학습시킬 값들 담기 (Batch size는 한 번에 학습시킬 size)\n",
        "- features, labels 는 실재 학습에 쓰일 Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pACuucOU3pn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices((x_train,y_train)).batch(len(x_train))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ieHLA8w4VCFI",
        "colab_type": "text"
      },
      "source": [
        "가설 검증을 통해 Softmax Classification 모델 만들기\n",
        "- W와 b는 학습을 통해 생성되는 모델에 쓰이는 Weight와 Bias\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBmONI7ZVQD_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "W = tf.Variable(tf.random.normal((3,3)))\n",
        "b = tf.Variable(tf.random.normal((3,)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aoojv7CwVc-x",
        "colab_type": "text"
      },
      "source": [
        "Softmax 함수를 통해 가장 높은 값을 구한다 (0~1 사이의 값 합계는 1)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9_jfQ6hVig7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def softmax_fn(features):\n",
        "    hypothesis = tf.nn.softmax(tf.matmul(features,W) + b)\n",
        "    return hypothesis"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6rOdmZU-V52s",
        "colab_type": "text"
      },
      "source": [
        "Cost 함수 정의\n",
        "- Cross Entropy loss 사용"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ilJYt5xV6_X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss_fn(hypothesis, features, labels):\n",
        "    cost = tf.reduce_mean(-tf.reduce_sum(labels * tf.math.log(hypothesis), axis = 1))\n",
        "    return cost"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HE4iHLIMWsPy",
        "colab_type": "text"
      },
      "source": [
        "Learning Rate 값 조정위한 Learning Decay 설정\n",
        "- starter_learning_rate : 최초 학습시 사용될 learning rate\n",
        "- global_step : 현재 학습 횟수\n",
        "- 적용 유무 decayed_learning_rate = learning_rate * decay_rate ^ (global_step / decay_steps)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rF4ptOOyXM0g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "is_decay = True\n",
        "starter_learning_rate = 0.1\n",
        "if(is_decay):\n",
        "    learning_rate = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=starter_learning_rate, decay_steps=100, decay_rate=0.96, staircase=True)\n",
        "    optimizer = tf.keras.optimizers.SGD(learning_rate)\n",
        "else:\n",
        "    optimizer = tf.keras.optimizers.SGD(learning_rate=starter_learning_rate)\n",
        "\n",
        "def grad(hypothesis, features, labels):\n",
        "    with tf.GradientTape() as tape :\n",
        "        loss_value = loss_fn(softmax_fn(features), features, labels)\n",
        "    return tape.gradient(loss_value, [W,b])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xe_FU9NYZqa",
        "colab_type": "text"
      },
      "source": [
        "정확도 측정"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U1iN17T1YbSf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def accuracy_fn(hypothesis, labels):\n",
        "    prediction = tf.argmax(hypothesis, 1)\n",
        "    is_correct = tf.equal(prediction, tf.argmax(labels, 1))\n",
        "    accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
        "    return accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oy8cc-hPYvxW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Epochs = 1001\n",
        "\n",
        "for step in range(Epochs):\n",
        "    for features, labels in iter(dataset):\n",
        "        features = tf.cast(features, tf.float32)\n",
        "        labels = tf.cast(labels, tf.float32)\n",
        "        grads = grad(softmax_fn(features), features, labels)\n",
        "        optimizer.apply_gradients(grads_and_vars=zip(grads,[W,b]))\n",
        "        if step % 100 == 0:\n",
        "            print(\"Iter : {}, Loss : {:4f}\".format(step, loss_fn(softmax_fn(features), features, labels)))\n",
        "x_test = tf.cast(x_test, tf.float32)\n",
        "y_test = tf.cast(y_test, tf.float32)\n",
        "test_acc = accuracy_fn(softmax_fn(x_test), y_test)\n",
        "print(\"Testset Accuracy: {:4f}\".format(test_acc))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
